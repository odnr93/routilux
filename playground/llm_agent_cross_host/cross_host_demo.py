"""
Complete demonstration of cross-host LLM agent workflow.

This module demonstrates:
1. Host A: Execute workflow, pause when LLM needs user input, save to cloud storage
2. Host B: Load from cloud storage, resume execution with user input

Execution Flow Overview:
========================

Host A (Initial Execution):
---------------------------
1. Create Flow with LLM Agent Routine
2. Execute flow with task
3. LLM Agent processes task
4. LLM generates question, needs user input
5. Routine pauses execution
6. Execution state saved to cloud storage
7. JobState status = "paused"

Host B (Resume Execution):
---------------------------
1. Load Flow and JobState from cloud storage
2. Deserialize both objects
3. Resume execution (flow.resume())
4. Deferred events are automatically emitted
5. Trigger user input handler with user response
6. LLM processes user response
7. Workflow completes
8. JobState status = "completed"
"""

import time
from routilux import Flow, JobState
from playground.llm_agent_cross_host.llm_agent_routine import LLMAgentRoutine
from playground.llm_agent_cross_host.mock_storage import get_storage
from playground.llm_agent_cross_host.mock_llm import get_llm_service, set_llm_service, MockLLMService
from playground.llm_agent_cross_host.logger import get_logger, set_logger, PlaygroundLogger


def create_flow() -> Flow:
    """Create a flow with LLM agent routine.
    
    Flow Structure:
    - Flow ID: "llm_agent_workflow"
    - Single Routine: LLMAgentRoutine
      * Slots: trigger, user_input, continue
      * Events: output, question, completed
    
    Returns:
        Tuple of (Flow, agent_routine_id)
    """
    logger = get_logger()
    logger.debug("FLOW", "åˆ›å»ºFlowå¯¹è±¡", flow_id="llm_agent_workflow")
    
    flow = Flow(flow_id="llm_agent_workflow")
    
    # Create LLM agent routine
    logger.debug("FLOW", "åˆ›å»ºLLM Agent Routine")
    agent = LLMAgentRoutine()
    agent_id = flow.add_routine(agent, "llm_agent")
    logger.debug("FLOW", "Routineå·²æ·»åŠ åˆ°Flow", routine_id=agent_id)
    
    return flow, agent_id


def host_a_execute_and_save(flow: Flow, entry_id: str, task: str = "åˆ†æç”¨æˆ·æ•°æ®"):
    """Host A: Execute workflow and save to cloud storage when paused.
    
    Execution Steps:
    1. Execute flow with task
    2. Flow starts event loop
    3. Trigger slot handler called
    4. LLM Agent processes task
    5. LLM generates question
    6. Routine pauses execution
    7. State saved to cloud storage
    8. JobState status = "paused"
    
    Args:
        flow: Flow object.
        entry_id: Entry routine ID.
        task: Task to process.
    
    Returns:
        JobState object.
    """
    logger = get_logger()
    logger.section("ğŸŒ Host A: æ‰§è¡Œå·¥ä½œæµå¹¶ä¿å­˜åˆ°äº‘å­˜å‚¨")
    
    # Execute flow
    logger.step(1, f"å¼€å§‹æ‰§è¡Œä»»åŠ¡: {task}")
    logger.debug("EXECUTION", "è°ƒç”¨flow.execute()", entry_id=entry_id, task=task)
    
    job_state = flow.execute(entry_id, entry_params={"task": task})
    
    logger.debug("EXECUTION", "execute()è¿”å›", 
                job_id=job_state.job_id,
                status=job_state.status)
    
    # Wait for execution (may pause)
    # Note: execute() is synchronous and waits for completion or pause
    logger.step(2, "ç­‰å¾…æ‰§è¡Œå®Œæˆæˆ–æš‚åœ...")
    max_wait = 10.0
    start_time = time.time()
    
    # Check status immediately after execute
    logger.info("STATE", f"åˆå§‹çŠ¶æ€: {job_state.status}")
    
    while time.time() - start_time < max_wait:
        current_status = job_state.status
        if current_status in ["completed", "failed", "paused", "cancelled"]:
            logger.state_change("running", current_status)
            break
        time.sleep(0.1)
    
    # Additional wait to ensure pause is processed
    if job_state.status == "running":
        logger.debug("EXECUTION", "æ‰§è¡Œä»åœ¨è¿è¡Œï¼Œç­‰å¾…æš‚åœå¤„ç†...")
        time.sleep(0.5)  # Give pause time to process
    
    # Check if paused
    if job_state.status == "paused":
        logger.step(3, "æ‰§è¡Œå·²æš‚åœï¼Œæ£€æŸ¥çŠ¶æ€")
        logger.info("STATE", f"â¸ï¸  æ‰§è¡Œå·²æš‚åœ")
        
        last_pause = job_state.pause_points[-1]
        logger.info("STATE", f"æš‚åœåŸå› : {last_pause['reason']}")
        logger.debug("STATE", "æš‚åœæ£€æŸ¥ç‚¹", checkpoint=last_pause['checkpoint'])
        
        # Get storage key
        storage_key = job_state.get_shared_data("storage_key")
        if storage_key:
            logger.info("STORAGE", f"å­˜å‚¨é”®: {storage_key}")
        else:
            # Manually save if not auto-saved
            logger.warning("STORAGE", "æœªæ‰¾åˆ°å­˜å‚¨é”®ï¼Œæ‰‹åŠ¨ä¿å­˜...")
            storage_key = f"execution_state/{job_state.job_id}"
            flow_data = flow.serialize()
            job_state_data = job_state.serialize()
            storage = get_storage()
            storage.put(storage_key, {
                "flow": flow_data,
                "job_state": job_state_data,
            })
            logger.info("STORAGE", f"å·²æ‰‹åŠ¨ä¿å­˜åˆ°: {storage_key}")
        
        # Display shared data
        logger.subsection("ğŸ“Š å…±äº«æ•°æ®")
        shared_data = job_state.shared_data
        for key, value in shared_data.items():
            logger.debug("STATE", f"  {key}: {value}")
        
        # Display execution history
        logger.subsection(f"ğŸ“œ æ‰§è¡Œå†å² ({len(job_state.execution_history)} æ¡)")
        for record in job_state.execution_history[-5:]:  # Show last 5
            logger.debug("EXECUTION", f"  [{record.timestamp}] {record.routine_id}.{record.event_name}")
    
    elif job_state.status == "completed":
        logger.info("EXECUTION", f"âœ… æ‰§è¡Œå·²å®Œæˆ")
        logger.info("EXECUTION", f"æ‰§è¡Œå†å²: {len(job_state.execution_history)} æ¡è®°å½•")
    
    return job_state


def host_b_load_and_resume(storage_key: str, user_response: str):
    """Host B: Load from cloud storage and resume execution.
    
    Execution Steps:
    1. Load Flow and JobState from cloud storage
    2. Deserialize Flow (workflow definition)
    3. Deserialize JobState (execution state)
    4. Verify deserialized data
    5. Set user response in shared_data
    6. Resume execution (flow.resume())
    7. Deferred events are automatically emitted
    8. Trigger user input handler
    9. Wait for completion
    10. Display final state
    
    Args:
        storage_key: Storage key from Host A.
        user_response: User response to continue execution.
    
    Returns:
        Resumed JobState object.
    """
    logger = get_logger()
    logger.section("ğŸŒ Host B: ä»äº‘å­˜å‚¨åŠ è½½å¹¶æ¢å¤æ‰§è¡Œ")
    
    # Load from cloud storage
    logger.step(1, f"ä»äº‘å­˜å‚¨åŠ è½½: {storage_key}")
    storage = get_storage()
    transfer_data = storage.get(storage_key)
    
    if not transfer_data:
        logger.error("STORAGE", f"å­˜å‚¨é”®æœªæ‰¾åˆ°: {storage_key}")
        raise ValueError(f"Storage key not found: {storage_key}")
    
    logger.debug("STORAGE", "æ•°æ®åŠ è½½å®Œæˆ", 
                has_flow="flow" in transfer_data,
                has_job_state="job_state" in transfer_data)
    
    # Deserialize Flow
    logger.step(2, "ååºåˆ—åŒ– Flow (å·¥ä½œæµå®šä¹‰)")
    flow = Flow()
    flow.deserialize(transfer_data["flow"])
    logger.info("FLOW", f"Flow ID: {flow.flow_id}")
    logger.info("FLOW", f"Routines: {list(flow.routines.keys())}")
    
    # Deserialize JobState
    logger.step(3, "ååºåˆ—åŒ– JobState (æ‰§è¡ŒçŠ¶æ€)")
    job_state = JobState()
    job_state.deserialize(transfer_data["job_state"])
    logger.info("STATE", f"Job ID: {job_state.job_id}")
    logger.state_change("paused", job_state.status, "ååºåˆ—åŒ–å")
    logger.info("STATE", f"Flow ID: {job_state.flow_id}")
    
    if job_state.pause_points:
        last_pause = job_state.pause_points[-1]
        logger.info("STATE", f"æœ€åæš‚åœåŸå› : {last_pause['reason']}")
        logger.debug("STATE", "æš‚åœæ£€æŸ¥ç‚¹", checkpoint=last_pause['checkpoint'])
    
    # Display shared data
    logger.subsection("ğŸ“Š å…±äº«æ•°æ® (æ¢å¤å‰)")
    shared_data = job_state.shared_data
    for key, value in shared_data.items():
        logger.debug("STATE", f"  {key}: {value}")
    
    # Set user response in shared data
    logger.step(4, f"è®¾ç½®ç”¨æˆ·å“åº”: {user_response}")
    job_state.update_shared_data("user_response", user_response)
    logger.debug("STATE", "ç”¨æˆ·å“åº”å·²ä¿å­˜åˆ°shared_data")
    
    # Resume execution
    logger.step(5, "æ¢å¤æ‰§è¡Œ (flow.resume())")
    logger.debug("EXECUTION", "resume()å°†æ‰§è¡Œä»¥ä¸‹æ“ä½œ:")
    logger.debug("EXECUTION", "  - è®¾ç½®job_state.status = 'running'")
    logger.debug("EXECUTION", "  - ååºåˆ—åŒ–pending_tasks")
    logger.debug("EXECUTION", "  - è‡ªåŠ¨emit deferred_events")
    logger.debug("EXECUTION", "  - å°†pending_tasksåŠ å…¥é˜Ÿåˆ—")
    logger.debug("EXECUTION", "  - å¯åŠ¨event_loop")
    
    resumed_job_state = flow.resume(job_state)
    logger.state_change("paused", resumed_job_state.status, "resume()å")
    
    # Trigger user input handler
    # Find LLM agent routine
    logger.step(6, "è§¦å‘ç”¨æˆ·è¾“å…¥å¤„ç†")
    agent_routine_id = None
    agent_routine = None
    for rid, routine in flow.routines.items():
        if isinstance(routine, LLMAgentRoutine):
            agent_routine_id = rid
            agent_routine = routine
            break
    
    if agent_routine:
        logger.info("ROUTINE", f"æ‰¾åˆ°LLM Agent Routine: {agent_routine_id}")
        user_input_slot = agent_routine.get_slot("user_input")
        if user_input_slot:
            logger.debug("EVENT", "è°ƒç”¨user_input slot handler", 
                        routine_id=agent_routine_id,
                        user_response=user_response)
            # Use receive() instead of call_handler() to properly set up execution context
            # receive() accepts job_state and flow parameters to set up the context
            user_input_slot.receive({"user_response": user_response}, 
                                   job_state=resumed_job_state, 
                                   flow=flow)
        else:
            logger.warning("ROUTINE", "æœªæ‰¾åˆ°user_input slot")
    else:
        logger.warning("ROUTINE", "æœªæ‰¾åˆ°LLM Agent Routine")
    
    # Wait for completion
    logger.step(7, "ç­‰å¾…æ‰§è¡Œå®Œæˆ...")
    max_wait = 10.0
    start_time = time.time()
    
    while time.time() - start_time < max_wait:
        if resumed_job_state.status in ["completed", "failed", "cancelled"]:
            logger.state_change("running", resumed_job_state.status)
            break
        time.sleep(0.1)
    
    # Display final state
    logger.subsection("ğŸ“Š æœ€ç»ˆçŠ¶æ€")
    logger.info("STATE", f"çŠ¶æ€: {resumed_job_state.status}")
    logger.info("STATE", f"æ‰§è¡Œå†å²: {len(resumed_job_state.execution_history)} æ¡è®°å½•")
    
    # Display final shared data
    logger.subsection("ğŸ“Š æœ€ç»ˆå…±äº«æ•°æ®")
    final_shared_data = resumed_job_state.shared_data
    for key, value in final_shared_data.items():
        logger.debug("STATE", f"  {key}: {value}")
    
    # Display routine state
    if agent_routine_id:
        routine_state = resumed_job_state.get_routine_state(agent_routine_id)
        if routine_state:
            logger.subsection(f"ğŸ“‹ RoutineçŠ¶æ€ ({agent_routine_id})")
            for key, value in routine_state.items():
                logger.debug("STATE", f"  {key}: {value}")
    
    return resumed_job_state


def main():
    """Main demonstration function.
    
    Complete Demo Flow:
    ==================
    
    Phase 1: Initialization
    -----------------------
    1. Initialize logger
    2. Initialize LLM service (mock)
    3. Initialize cloud storage (mock)
    4. Create Flow with LLM Agent Routine
    
    Phase 2: Host A Execution
    -------------------------
    5. Execute flow with task
    6. LLM processes task and generates question
    7. Routine pauses execution
    8. State saved to cloud storage
    
    Phase 3: Host B Recovery
    -------------------------
    9. Load Flow and JobState from cloud storage
    10. Resume execution
    11. Process user input
    12. Complete workflow
    
    Phase 4: Summary
    ----------------
    13. Display execution summary
    """
    logger = get_logger()
    logger.section("ğŸš€ LLMæ™ºèƒ½ä½“è·¨ä¸»æœºä¸­æ–­æ¢å¤ç³»ç»Ÿæ¼”ç¤º")
    
    # Initialize services
    logger.subsection("ğŸ”§ åˆå§‹åŒ–æœåŠ¡")
    logger.step(1, "åˆå§‹åŒ–LLMæœåŠ¡ (æ¨¡æ‹Ÿ)")
    llm_service = MockLLMService(delay=0.05)  # Faster for demo
    set_llm_service(llm_service)
    logger.info("SERVICE", "âœ… LLMæœåŠ¡å·²åˆå§‹åŒ–", model=llm_service.model)
    
    logger.step(2, "åˆå§‹åŒ–äº‘å­˜å‚¨æœåŠ¡ (æ¨¡æ‹Ÿ)")
    logger.info("SERVICE", "âœ… äº‘å­˜å‚¨æœåŠ¡å·²åˆå§‹åŒ–")
    
    # Create flow
    logger.subsection("ğŸ“ åˆ›å»ºå·¥ä½œæµ")
    logger.step(3, "åˆ›å»ºFlowå’ŒLLM Agent Routine")
    flow, agent_id = create_flow()
    logger.info("FLOW", f"Flow ID: {flow.flow_id}")
    logger.info("FLOW", f"Agent Routine ID: {agent_id}")
    
    # ============================================
    # Host A: Execute and save
    # ============================================
    logger.subsection("Phase 2: Host A æ‰§è¡Œ")
    job_state = host_a_execute_and_save(flow, agent_id, task="åˆ†æç”¨æˆ·è¡Œä¸ºæ•°æ®")
    
    # Check if paused
    if job_state.status == "paused":
        storage_key = job_state.get_shared_data("storage_key")
        if not storage_key:
            storage_key = f"execution_state/{job_state.job_id}"
        
        # ============================================
        # Host B: Load and resume
        # ============================================
        logger.subsection("Phase 3: Host B æ¢å¤")
        user_response = "æˆ‘é€‰æ‹©æ–¹æ¡ˆAï¼šç»§ç»­è¯¦ç»†åˆ†æ"
        resumed_job_state = host_b_load_and_resume(storage_key, user_response)
        
        # Summary
        logger.subsection("Phase 4: æ‰§è¡Œæ€»ç»“")
        logger.section("ğŸ“Š æ‰§è¡Œæ€»ç»“")
        logger.info("SUMMARY", f"åˆå§‹æ‰§è¡ŒçŠ¶æ€: {job_state.status}")
        logger.info("SUMMARY", f"æ¢å¤åçŠ¶æ€: {resumed_job_state.status}")
        logger.info("SUMMARY", f"æ€»æ‰§è¡Œå†å²è®°å½•: {len(resumed_job_state.execution_history)}")
        logger.info("SUMMARY", f"æš‚åœæ¬¡æ•°: {len(resumed_job_state.pause_points)}")
        logger.info("SUMMARY", f"å…±äº«æ—¥å¿—æ¡ç›®: {len(resumed_job_state.shared_log)}")
        logger.info("SUMMARY", "âœ… æ¼”ç¤ºå®Œæˆï¼")
    else:
        logger.warning("EXECUTION", "æ‰§è¡Œæœªæš‚åœï¼Œæ— éœ€è·¨ä¸»æœºæ¢å¤")
        logger.info("EXECUTION", f"æœ€ç»ˆçŠ¶æ€: {job_state.status}")


if __name__ == "__main__":
    # Initialize logger with verbose mode
    logger = PlaygroundLogger(verbose=True, show_timestamps=True)
    set_logger(logger)
    
    main()

